{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces basic ideas in pre-processing text using minutes from the Bank of England's Monetary Policy Committee meetings as an example, as well as information retrieval via dictionary methods.  For a more extensive discussion of pre-processing, see the notebook in the \"text-mining-tutorial\" repository.\n",
    "\n",
    "Apart from the standard numpy and pandas packages, the tutorial also relies on the topicmodels package, which can be installed using ```pip install topic-modelling-tools```, see https://github.com/sekhansen/text-mining-tutorial for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import topicmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the minutes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_table(\"uk_data/mpc_minutes.txt\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7277, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'year', u'minutes'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 7,277 paragraphs along with a meeting identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199706"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.year.values[0] # first year in sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201410"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.year.values[-1] # last year in sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'  The 12-month growth rate of notes and coins had fallen back since January, when it was 7.1%. It had fallen to 6.3% in April and the provisional estimate for May was 6.1%. It was not yet clear whether the fall simply reflected a deceleration in demand for cash following the recent fall in retail price inflation, or whether it had implications for future spending.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.minutes.values[1] # second paragraph in sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook, we aggregate the data to the full meeting rather than paragraph level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_agg = data.groupby('year').agg(lambda x: ' '.join(x))\n",
    "data_agg.shape[0] # total number of meetings in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Pre-Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in pre-processing is to tokenize the data.  Tokenization breaks a raw character string into individual 'tokens' based on some pre-defined rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'section',\n",
       " u'i',\n",
       " u'of',\n",
       " u'this',\n",
       " u'minute',\n",
       " u'summarises',\n",
       " u'the',\n",
       " u'analysis',\n",
       " u'presented',\n",
       " u'to',\n",
       " u'the',\n",
       " u'mpc',\n",
       " u'by',\n",
       " u'bank',\n",
       " u'staff',\n",
       " u',',\n",
       " u'and',\n",
       " u'also',\n",
       " u'incorporates',\n",
       " u'information',\n",
       " u'that',\n",
       " u'became',\n",
       " u'available',\n",
       " u'to',\n",
       " u'the',\n",
       " u'committee',\n",
       " u'after',\n",
       " u'the',\n",
       " u'presentation']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsobj = topicmodels.RawDocs(data_agg.minutes, \"long\") # creates object for pre-processing\n",
    "docsobj.tokens[1][1:30] # first thirty tokens of first meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step in pre-processing is to remove all non-alphabetic tokens and all tokens of length one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'of',\n",
       " u'this',\n",
       " u'minute',\n",
       " u'summarises',\n",
       " u'the',\n",
       " u'analysis',\n",
       " u'presented',\n",
       " u'to',\n",
       " u'the',\n",
       " u'mpc',\n",
       " u'by',\n",
       " u'bank',\n",
       " u'staff',\n",
       " u'and',\n",
       " u'also',\n",
       " u'incorporates',\n",
       " u'information',\n",
       " u'that',\n",
       " u'became',\n",
       " u'available',\n",
       " u'to',\n",
       " u'the',\n",
       " u'committee',\n",
       " u'after',\n",
       " u'the',\n",
       " u'presentation',\n",
       " u'section',\n",
       " u'ii',\n",
       " u'summarises']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsobj.token_clean(1)\n",
    "docsobj.tokens[1][1:30] # first thirty tokens of first meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to keep track of the dimensionality of the data as we go through different pre-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens = 8967\n",
      "number of total tokens = 1115996\n"
     ]
    }
   ],
   "source": [
    "all_stems = [s for d in docsobj.tokens for s in d]\n",
    "print(\"number of unique tokens = %d\" % len(set(all_stems)))\n",
    "print(\"number of total tokens = %d\" % len(all_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in pre-processing is to remove stopwords, which here have been defined by the \"long\" argument to RawDocs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'a',\n",
       " u'about',\n",
       " u'above',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'all',\n",
       " u'also',\n",
       " u'am',\n",
       " u'an',\n",
       " u'and',\n",
       " u'another',\n",
       " u'any',\n",
       " u'are',\n",
       " u'as',\n",
       " u'at',\n",
       " u'back',\n",
       " u'be',\n",
       " u'because',\n",
       " u'been',\n",
       " u'before',\n",
       " u'being',\n",
       " u'below',\n",
       " u'between',\n",
       " u'both',\n",
       " u'but',\n",
       " u'by',\n",
       " u'could',\n",
       " u'did',\n",
       " u'do',\n",
       " u'does',\n",
       " u'doing',\n",
       " u'down',\n",
       " u'during',\n",
       " u'each',\n",
       " u'even',\n",
       " u'ever',\n",
       " u'every',\n",
       " u'few',\n",
       " u'first',\n",
       " u'five',\n",
       " u'for',\n",
       " u'four',\n",
       " u'from',\n",
       " u'further',\n",
       " u'get',\n",
       " u'go',\n",
       " u'goes',\n",
       " u'had',\n",
       " u'has',\n",
       " u'have',\n",
       " u'having',\n",
       " u'he',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'high',\n",
       " u'him',\n",
       " u'himself',\n",
       " u'his',\n",
       " u'how',\n",
       " u'however',\n",
       " u'i',\n",
       " u'if',\n",
       " u'in',\n",
       " u'into',\n",
       " u'is',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'just',\n",
       " u'least',\n",
       " u'less',\n",
       " u'like',\n",
       " u'long',\n",
       " u'made',\n",
       " u'make',\n",
       " u'many',\n",
       " u'me',\n",
       " u'more',\n",
       " u'most',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'never',\n",
       " u'new',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'now',\n",
       " u'of',\n",
       " u'off',\n",
       " u'old',\n",
       " u'on',\n",
       " u'once',\n",
       " u'one',\n",
       " u'only',\n",
       " u'or',\n",
       " u'other',\n",
       " u'ought',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u'put',\n",
       " u'said',\n",
       " u'same',\n",
       " u'say',\n",
       " u'says',\n",
       " u'second',\n",
       " u'see',\n",
       " u'seen',\n",
       " u'she',\n",
       " u'should',\n",
       " u'since',\n",
       " u'so',\n",
       " u'some',\n",
       " u'still',\n",
       " u'such',\n",
       " u'take',\n",
       " u'than',\n",
       " u'that',\n",
       " u'the',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'them',\n",
       " u'themselves',\n",
       " u'then',\n",
       " u'there',\n",
       " u'these',\n",
       " u'they',\n",
       " u'this',\n",
       " u'those',\n",
       " u'three',\n",
       " u'through',\n",
       " u'to',\n",
       " u'too',\n",
       " u'two',\n",
       " u'under',\n",
       " u'until',\n",
       " u'up',\n",
       " u'us',\n",
       " u'very',\n",
       " u'was',\n",
       " u'way',\n",
       " u'we',\n",
       " u'well',\n",
       " u'were',\n",
       " u'what',\n",
       " u'when',\n",
       " u'where',\n",
       " u'whether',\n",
       " u'which',\n",
       " u'while',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'why',\n",
       " u'with',\n",
       " u'would',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsobj.stopwords # the stopwords removed in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens = 8818\n",
      "number of total tokens = 613560\n"
     ]
    }
   ],
   "source": [
    "docsobj.stopword_remove(\"tokens\")\n",
    "\n",
    "all_stems = [s for d in docsobj.tokens for s in d]\n",
    "print(\"number of unique tokens = %d\" % len(set(all_stems)))\n",
    "print(\"number of total tokens = %d\" % len(all_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the effect on the number of total tokens from removing a relatively small number of unique tokens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final pre-processing step in this example is stemming, which removes suffixes from words in order to map tokens with different grammatical forms into a single linguistic root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique stems = 5550\n",
      "number of total stems = 613560\n"
     ]
    }
   ],
   "source": [
    "docsobj.stem()\n",
    "\n",
    "all_stems = [s for d in docsobj.stems for s in d]\n",
    "print(\"number of unique stems = %d\" % len(set(all_stems)))\n",
    "print(\"number of total stems = %d\" % len(all_stems))\n",
    "\n",
    "docsobj.stopword_remove(\"stems\") # remove stems that are on the stopword list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the total number of terms has stayed the same, but the number of unique terms is much less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Dictionary Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we use the monetary policy sentiment dictionaries from Apel and Blix-Grimaldi (2012) to characterize the sentiment of each MPC meeting.  We will then compare the measured sentiment to UK GDP as measured by the Office for National Statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bowobj = topicmodels.BOW(docsobj.stems) # create an object for bag-of-words analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'accelerate',\n",
       " u'accelerated',\n",
       " u'accelerates',\n",
       " u'accelerating',\n",
       " u'expand',\n",
       " u'expanded',\n",
       " u'expanding',\n",
       " u'expands',\n",
       " u'fast',\n",
       " u'faster',\n",
       " u'fastest',\n",
       " u'gain',\n",
       " u'gained',\n",
       " u'gaining',\n",
       " u'gains',\n",
       " u'high',\n",
       " u'higher',\n",
       " u'highest',\n",
       " u'increase',\n",
       " u'increased',\n",
       " u'increases',\n",
       " u'increasing',\n",
       " u'strong',\n",
       " u'stronger',\n",
       " u'strongest'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicmodels.bow_data.pos_dict # the positive sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'contract',\n",
       " u'contracted',\n",
       " u'contracting',\n",
       " u'contracts',\n",
       " u'decelerate',\n",
       " u'decelerated',\n",
       " u'decelerates',\n",
       " u'decelerating',\n",
       " u'decrease',\n",
       " u'decreased',\n",
       " u'decreases',\n",
       " u'decreasing',\n",
       " u'lose',\n",
       " u'losing',\n",
       " u'loss',\n",
       " u'losses',\n",
       " u'lost',\n",
       " u'low',\n",
       " u'lower',\n",
       " u'lowest',\n",
       " u'slow',\n",
       " u'slower',\n",
       " u'slowest',\n",
       " u'weak',\n",
       " u'weaker',\n",
       " u'weakest'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicmodels.bow_data.neg_dict # the negative sentiment words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall sentiment indicator is formed of the net count of positive words divided by the total number of sentiment words (positive + negative).  All sentiment words are stemmed in order to match the data we formed in pre-preprocessing above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_agg['pos'] = bowobj.pos_count('stems')\n",
    "data_agg['neg'] = bowobj.neg_count('stems')\n",
    "data_agg['sentiment'] = (data_agg.pos - data_agg.neg) /\\\n",
    "                        (data_agg.pos + data_agg.neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add quarterly GDP data collected from the ONS website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ons = pd.read_csv('uk_data/ons_quarterly_gdp.csv')\n",
    "data_agg['gdp_growth'] = ons.gdp_growth.values\n",
    "data_agg['quarter'] = ons.quarter.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the average MPC minutes sentiment per quarter, and correlated with GDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 pos       neg  sentiment  gdp_growth\n",
      "pos         1.000000  0.536057   0.572704    0.382814\n",
      "neg         0.536057  1.000000  -0.289257    0.153855\n",
      "sentiment   0.572704 -0.289257   1.000000    0.409506\n",
      "gdp_growth  0.382814  0.153855   0.409506    1.000000\n"
     ]
    }
   ],
   "source": [
    "temp = data_agg.groupby('quarter').agg(np.mean)\n",
    "print(temp.corr())\n",
    "temp['quarter'] = sorted(set(ons.label))\n",
    "temp[['quarter', 'sentiment', 'gdp_growth']].to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spite of its arguable lack of subtlety, here dictionary methods have produced a sentiment indicator that indeed correlates with real activity.  Further exploration can be done with the output file printed above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
